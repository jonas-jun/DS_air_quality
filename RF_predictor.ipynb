{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# args\n",
    "opt = EasyDict()\n",
    "opt.features = ['no2', 'co', 'so2', 'pm25_con', 'temp', 'wind_direction', 'cloud', 'precipitation',\n",
    "                'pressure', 'wind_speed_rs', 'gust_rs', 'overall_int', 'pm25_cat']\n",
    "opt.seed = 42\n",
    "opt.dataset = 1 # 1 for ml(past pm), 2 for dl(seq)\n",
    "opt.test_ratio = 0.3 # 0.2 for dl models 0.3 for ml models\n",
    "opt.val_ratio = 0.2 # for dl models\n",
    "opt.batch_size = None # for dl models\n",
    "opt.num_epochs = None # for dl models\n",
    "opt.log_steps = None # for dl models\n",
    "opt.patience = 5 # for dl models\n",
    "opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "opt.model_name = 'dt' # ml: [dt, rf], dl: [full, each] (encoder)\n",
    "opt.num_classes = 4 # for dl models\n",
    "opt.num_layers = None # for dl models\n",
    "\n",
    "print(opt.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.model_name in ['dt', 'rf']:\n",
    "    df = pd.read_csv('dataset/for_ML.csv')\n",
    "    df = df[opt.features]\n",
    "else:\n",
    "    df = pd.read_csv('dataset/for_Seq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29083, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no2</th>\n",
       "      <th>co</th>\n",
       "      <th>so2</th>\n",
       "      <th>pm25_con</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>cloud</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_speed_rs</th>\n",
       "      <th>gust_rs</th>\n",
       "      <th>overall_int</th>\n",
       "      <th>pm25_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.094571</td>\n",
       "      <td>-0.575601</td>\n",
       "      <td>-0.579114</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-1.653282</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.556088</td>\n",
       "      <td>-0.575601</td>\n",
       "      <td>-0.579114</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-1.529149</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.622019</td>\n",
       "      <td>-0.575601</td>\n",
       "      <td>-1.076117</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-1.529149</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        no2        co       so2  pm25_con  temp  wind_direction  cloud  \\\n",
       "0 -0.094571 -0.575601 -0.579114      15.0  20.0               2   75.0   \n",
       "1 -0.556088 -0.575601 -0.579114      14.0  20.0               2   98.0   \n",
       "2 -0.622019 -0.575601 -1.076117      13.0  20.0               2  100.0   \n",
       "\n",
       "   precipitation  pressure  wind_speed_rs  gust_rs  overall_int  pm25_cat  \n",
       "0            3.5 -1.653282            8.0     12.0            0         0  \n",
       "1            0.6 -1.529149            8.0     12.0            1         0  \n",
       "2            1.3 -1.529149            9.0     13.0            1         1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {0: 'good', 1: 'moderate', 2: 'bad', 3: 'worst'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29083 entries, 0 to 29082\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   no2             29083 non-null  float64\n",
      " 1   co              29083 non-null  float64\n",
      " 2   so2             29083 non-null  float64\n",
      " 3   pm25_con        29083 non-null  float64\n",
      " 4   temp            29083 non-null  float64\n",
      " 5   wind_direction  29083 non-null  int64  \n",
      " 6   cloud           29083 non-null  float64\n",
      " 7   precipitation   29083 non-null  float64\n",
      " 8   pressure        29083 non-null  float64\n",
      " 9   wind_speed_rs   29083 non-null  float64\n",
      " 10  gust_rs         29083 non-null  float64\n",
      " 11  overall_int     29083 non-null  int64  \n",
      " 12  pm25_cat        29083 non-null  int64  \n",
      "dtypes: float64(10), int64(3)\n",
      "memory usage: 2.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['wind_direction', 'overall_int']\n",
    "for cat in category:\n",
    "    df[cat] = df[cat].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Stable Random Seed\n",
    "SEED = opt.seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 20,358 \n",
      "X_test: 8,725\n",
      "y_train: 20,358 \n",
      "y_test: 8,725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('pm25_cat', axis=1)\n",
    "y = df['pm25_cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=opt.test_ratio, random_state=SEED)\n",
    "print('X_train: {:,} \\nX_test: {:,}\\ny_train: {:,} \\ny_test: {:,}'.format(len(X_train), len(X_test), len(y_train),\n",
    "                                                                  len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 1, 1: 1, 2: 3, 3: 5}, max_depth=12,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=12, class_weight={0:1, 1:1, 2:3, 3:5}, random_state=SEED)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        good       0.85      0.78      0.82      2368\n",
      "    moderate       0.83      0.84      0.83      4573\n",
      "         bad       0.69      0.83      0.75      1510\n",
      "       worst       0.77      0.32      0.45       274\n",
      "\n",
      "    accuracy                           0.80      8725\n",
      "   macro avg       0.78      0.69      0.71      8725\n",
      "weighted avg       0.81      0.80      0.80      8725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_names = class_map.values()\n",
    "print(classification_report(y_test, predicted, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_good</th>\n",
       "      <th>pr_moderate</th>\n",
       "      <th>pr_bad</th>\n",
       "      <th>pr_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truth_good</th>\n",
       "      <td>1851</td>\n",
       "      <td>506</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truth_moderate</th>\n",
       "      <td>314</td>\n",
       "      <td>3828</td>\n",
       "      <td>429</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truth_bad</th>\n",
       "      <td>3</td>\n",
       "      <td>230</td>\n",
       "      <td>1253</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truth_worst</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>130</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pr_good  pr_moderate  pr_bad  pr_worst\n",
       "truth_good         1851          506      11         0\n",
       "truth_moderate      314         3828     429         2\n",
       "truth_bad             3          230    1253        24\n",
       "truth_worst           1           56     130        87"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_auc_score\n",
    "cf = pd.DataFrame(confusion_matrix(y_test, predicted))\n",
    "cf.index = ['truth_good', 'truth_moderate', 'truth_bad', 'truth_worst']\n",
    "cf.columns = ['pr_good', 'pr_moderate', 'pr_bad', 'pr_worst']\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pr_good           3\n",
       "pr_moderate     230\n",
       "pr_bad         1253\n",
       "Name: truth_bad, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.loc['truth_bad'][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(matrix):\n",
    "    bad = matrix.loc['truth_bad']\n",
    "    worst = matrix.loc['truth_worst']\n",
    "    t_bad = sum(bad)\n",
    "    t_worst = sum(worst)\n",
    "    right_bad = bad['pr_bad']\n",
    "    right_worst = worst['pr_worst']\n",
    "    return round((right_bad + right_worst) / (t_bad + t_worst), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7511"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_metric(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def custom_metric(matrix):\n",
    "    bad = matrix.loc['truth_bad'][:-1]\n",
    "    worst = matrix.loc['truth_worst'][:-1]\n",
    "    t_bad = sum(bad)\n",
    "    t_worst = sum(worst)\n",
    "    right_bad = bad['pr_bad']+bad['pr_worst']\n",
    "    right_worst = worst['pr_worst']+worst['pr_bad']\n",
    "    return round(((right_bad + right_worst) / (t_bad + t_worst)), 4)\n",
    "\n",
    "def modeling(depth, weight, X_train, y_train, X_test):\n",
    "    clf = RandomForestClassifier(max_depth=depth, class_weight=weight, random_state=SEED)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    return predicted\n",
    "\n",
    "def get_scores(y_test, predicted):\n",
    "    cf = pd.DataFrame(confusion_matrix(y_test, predicted))\n",
    "    cf.index = ['truth_good', 'truth_moderate', 'truth_bad', 'truth_worst']\n",
    "    cf.columns = ['pr_good', 'pr_moderate', 'pr_bad', 'pr_worst']\n",
    "    cf['truth_total'] = cf['pr_good']+cf['pr_moderate']+cf['pr_bad']+cf['pr_worst']\n",
    "    recall_bad = custom_metric(cf)\n",
    "    acc = round(accuracy_score(y_test, predicted), 4)\n",
    "    f1 = round(f1_score(y_test, predicted, average='macro'), 4)\n",
    "    \n",
    "    print(' >> recall_bad: {:.02f}%'.format(recall_bad*100))\n",
    "    print(' >> total acc.: {:.02f}%'.format(acc*100))\n",
    "    print(' >> total F1: {:.02f}'.format(f1*100))\n",
    "    return recall_bad, acc, f1, cf\n",
    "\n",
    "def run(X_train, y_train, X_test, y_test, depth, weight):\n",
    "    predicted = modeling(depth, weight, X_train, y_train, X_test)\n",
    "    print('max_depth: {:} | class_weight: {:}'.format(depth, weight))\n",
    "    _, _, _, cf = get_scores(y_test, predicted)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [None, {0:1, 1:1, 2:2, 3:3}, {0:1, 1:1, 2:3, 3:5}]\n",
    "depth = [8, 10, 12, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 8 | class_weight: None\n",
      " >> recall_bad: 73.26%\n",
      " >> total acc.: 80.56%\n",
      " >> total F1: 67.88\n",
      "max_depth: 10 | class_weight: None\n",
      " >> recall_bad: 73.32%\n",
      " >> total acc.: 81.00%\n",
      " >> total F1: 70.13\n",
      "max_depth: 12 | class_weight: None\n",
      " >> recall_bad: 73.88%\n",
      " >> total acc.: 80.80%\n",
      " >> total F1: 70.47\n",
      "max_depth: 15 | class_weight: None\n",
      " >> recall_bad: 74.22%\n",
      " >> total acc.: 80.77%\n",
      " >> total F1: 70.66\n",
      "max_depth: 8 | class_weight: {0: 1, 1: 1, 2: 2, 3: 3}\n",
      " >> recall_bad: 81.67%\n",
      " >> total acc.: 80.78%\n",
      " >> total F1: 71.27\n",
      "max_depth: 10 | class_weight: {0: 1, 1: 1, 2: 2, 3: 3}\n",
      " >> recall_bad: 80.77%\n",
      " >> total acc.: 81.02%\n",
      " >> total F1: 71.70\n",
      "max_depth: 12 | class_weight: {0: 1, 1: 1, 2: 2, 3: 3}\n",
      " >> recall_bad: 79.65%\n",
      " >> total acc.: 81.07%\n",
      " >> total F1: 71.60\n",
      "max_depth: 15 | class_weight: {0: 1, 1: 1, 2: 2, 3: 3}\n",
      " >> recall_bad: 77.69%\n",
      " >> total acc.: 80.99%\n",
      " >> total F1: 71.22\n",
      "max_depth: 8 | class_weight: {0: 1, 1: 1, 2: 3, 3: 5}\n",
      " >> recall_bad: 86.60%\n",
      " >> total acc.: 79.51%\n",
      " >> total F1: 70.49\n",
      "max_depth: 10 | class_weight: {0: 1, 1: 1, 2: 3, 3: 5}\n",
      " >> recall_bad: 85.59%\n",
      " >> total acc.: 80.09%\n",
      " >> total F1: 71.11\n",
      "max_depth: 12 | class_weight: {0: 1, 1: 1, 2: 3, 3: 5}\n",
      " >> recall_bad: 83.74%\n",
      " >> total acc.: 80.45%\n",
      " >> total F1: 71.26\n",
      "max_depth: 15 | class_weight: {0: 1, 1: 1, 2: 3, 3: 5}\n",
      " >> recall_bad: 79.99%\n",
      " >> total acc.: 80.83%\n",
      " >> total F1: 71.35\n"
     ]
    }
   ],
   "source": [
    "for i in weight:\n",
    "    for j in depth:\n",
    "        run(X_train, y_train, X_test, y_test, j, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "**The more class weight for 'worst', the better recall for bad air quality**  \n",
    "\n",
    "**Recall_bad: 86.60% | Acc: 79.51% | F1: 70.49** when max_depth=8, weight={1,1,3,5}  \n",
    "Best total ACC: 81.07% when max_depth=12, weight={1,1,2,3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> recall_bad: 86.60%\n",
      " >> total acc.: 79.51%\n",
      " >> total F1: 70.49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_good</th>\n",
       "      <th>pr_moderate</th>\n",
       "      <th>pr_bad</th>\n",
       "      <th>pr_worst</th>\n",
       "      <th>truth_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truth_good</th>\n",
       "      <td>1813</td>\n",
       "      <td>537</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truth_moderate</th>\n",
       "      <td>291</td>\n",
       "      <td>3739</td>\n",
       "      <td>541</td>\n",
       "      <td>2</td>\n",
       "      <td>4573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truth_bad</th>\n",
       "      <td>3</td>\n",
       "      <td>186</td>\n",
       "      <td>1300</td>\n",
       "      <td>21</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truth_worst</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>139</td>\n",
       "      <td>85</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pr_good  pr_moderate  pr_bad  pr_worst  truth_total\n",
       "truth_good         1813          537      18         0         2368\n",
       "truth_moderate      291         3739     541         2         4573\n",
       "truth_bad             3          186    1300        21         1510\n",
       "truth_worst           1           49     139        85          274"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = modeling(8, {0:1, 1:1, 2:3, 3:5}, X_train, y_train, X_test)\n",
    "recall_bad, acc, f1, cf = get_scores(y_test, predicted)\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
